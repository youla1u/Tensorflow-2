{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7075c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e8e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF Version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed692e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"eager execution:\",tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0777d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+2+3+4+5= tf.Tensor(15, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"1+2+3+4+5=\",tf.reduce_sum([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e27b66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7186d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(aconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f3a1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+2+3+4+5= tf.Tensor(12, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"1+2+3+4+5=\",tf.reduce_sum([1,2,4,5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e1f8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "aconst=tf.reduce_sum([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d991a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "aconst=tf.reduce_sum([1,aconst,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c3da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(aconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1258c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une constante TF 2 est une valeur immuable, et un exemple simple est montré ici :\n",
    "aconst = tf.constant(3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333797f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Une variable TF 2 est une \"valeur entraînable\" dans un graphe TF 2. Par exemple, la pente m et l'ordonnée à l'origine b\n",
    "#d'une ligne la mieux ajustée pour un ensemble de données composé de points dans le plan euclidien sont deux exemples de \n",
    "#valeurs entraînables.Quelques exemples de variables TF sont présentés ici :\n",
    "\n",
    "b = tf.Variable(3, name=\"b\")\n",
    "x = tf.Variable(2, name=\"x\")\n",
    "z = tf.Variable(5*x, name=\"z\")\n",
    "W = tf.Variable(20)\n",
    "lm = tf.Variable(W*x + b, name=\"lm\")\n",
    "\n",
    "# Notez que b, x et z sont définis comme des variables TF. De plus, b et x sont initialisés avec des valeurs numériques, \n",
    "# alors que la valeur de la variable z est une expression qui dépend de la valeur de x (qui vaut 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants in TF 2\n",
    "\n",
    "# Voici une courte liste de certaines propriétés des constantes TF 2 :\n",
    "\n",
    "# • initialisés lors de leur définition\n",
    "# • ne peut pas changer leur valeur (\"immuable\")\n",
    "# • peut spécifier son nom (facultatif)\n",
    "# • le type est obligatoire (ex : tf.float32)\n",
    "# • ne sont pas modifiés pendant l'entrainement\n",
    "\n",
    "# Le Listing 1.1 affiche le contenu de tf2_constants1.py, qui illustre comment affecter \n",
    "# et imprimer les valeurs de certaines constantes TF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d8e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 1.1: tf2_constants1.py\n",
    "\n",
    "scalar = tf.constant(10)\n",
    "vector = tf.constant([1,2,3,4,5])\n",
    "matrix = tf.constant([[1,2,3],[4,5,6]])\n",
    "cube = tf.constant([[[1],[2],[3]],[[4],[5],[6]],[[7],[8],[9]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7759a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(5,)\n",
      "(2, 3)\n",
      "(3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(scalar.get_shape())\n",
    "print(vector.get_shape())\n",
    "print(matrix.get_shape())\n",
    "print(cube.get_shape())\n",
    "\n",
    "# Le Listing 1.1 contient quatre instructions tf.constant() qui définissent les tenseurs TF 2 de dimension 0, 1, 2 et 3,\n",
    "# respectivement. La deuxième partie du Listing 1.1 contient quatre instructions print() qui affichent la forme des quatre\n",
    "# constantes TF 2 définies dans la première section du Listing 1.1. La sortie de la liste 1.1 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le Listing 1.2 affiche le contenu de tf2_constants2.py, qui illustre comment attribuer des valeurs aux \n",
    "# constantes TF 2 puis imprimer ces valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7548f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 1.2: tf2_constants2.p\n",
    "\n",
    "x = tf.constant(5,name=\"x\")\n",
    "y = tf.constant(8,name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "962174be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prod(x, y):\n",
    "    z = 2*x + 3*y\n",
    "    return z\n",
    "\n",
    "result = calc_prod(x, y)\n",
    "print('result =',result)\n",
    "\n",
    "# Le Listing 1.2 définit une fonction Python \"décorée\" (en gras) calc_prod() avec du code TF 2 qui serait autrement\n",
    "# inclus dans un bloc de code TF 1.x tf.Session(). Plus précisément, z serait inclus dans une sess. run() , ainsi \n",
    "# qu'un feed_dict qui fournit des valeurs pour x et y. Heureusement, une fonction Python décorée dans TF 2 fait \n",
    "# ressembler le code à du code Python \"normal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b4857a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v.value(): tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "v.numpy(): [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(\"v.value():\", v.value())\n",
    "print(\"\")\n",
    "print(\"v.numpy():\", v.numpy())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8de0106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 2., 42.,  6.],\n",
      "       [ 7.,  8.,  9.]], dtype=float32)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v.assign(2 * v)\n",
    "v[0, 1].assign(42)\n",
    "v[1].assign([7., 8., 9.])\n",
    "print(\"v:\",v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffca8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=tf.constant(3.0)\n",
    "B=tf.fill([2,3],5.0)\n",
    "C=tf.constant([3.0,4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7bd78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  tf.Tensor(0, shape=(), dtype=int32)\n",
      "B:  tf.Tensor(2, shape=(), dtype=int32)\n",
      "C:  tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def show_rank(x):\n",
    "    return tf.rank(x)\n",
    "\n",
    "print(\"A: \",show_rank(A))\n",
    "print(\"B: \",show_rank(B))\n",
    "print(\"C: \",show_rank(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b84de660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  ()\n",
      "b:  (2, 3)\n",
      "c:  (2, 3)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant(3.0)\n",
    "b=tf.fill([2,3],5.0)\n",
    "c=tf.constant([[1.0,2.0,3.0], [4.0,5.0,6.0]])\n",
    "\n",
    "print(\"a: \",a.get_shape())\n",
    "print(\"b: \",b.get_shape())\n",
    "print(\"c: \",c.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd0e8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5.],\n",
       "       [5., 5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82862b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_0: []\n",
      "tuple_0: ()\n",
      "list_1: [3]\n",
      "tuple_1: 3\n",
      "list_2: [3, 7]\n",
      "tuple_2: (3, 7)\n",
      "any_list1: [None]\n",
      "any_tuple1: None\n",
      "any_list2: [7, None]\n",
      "any_list3: [7, None, None]\n"
     ]
    }
   ],
   "source": [
    "# Listing 1.6: tf2_shapes.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "list_0 = []\n",
    "tuple_0 = ()\n",
    "print(\"list_0:\",list_0)\n",
    "print(\"tuple_0:\",tuple_0)\n",
    "\n",
    "list_1 = [3]\n",
    "tuple_1 = (3)\n",
    "print(\"list_1:\",list_1)\n",
    "print(\"tuple_1:\",tuple_1)\n",
    "\n",
    "list_2 = [3, 7]\n",
    "tuple_2 = (3, 7)\n",
    "print(\"list_2:\",list_2)\n",
    "print(\"tuple_2:\",tuple_2)\n",
    "\n",
    "any_list1 = [None]\n",
    "any_tuple1 = (None)\n",
    "print(\"any_list1:\",any_list1)\n",
    "print(\"any_tuple1:\",any_tuple1)\n",
    "\n",
    "any_list2 = [7,None]\n",
    "any_list3 = [7,None,None]\n",
    "print(\"any_list2:\",any_list2)\n",
    "print(\"any_list3:\",any_list3)\n",
    "\n",
    "# Le Listing 1.6 contient des listes simples et des tuples de différentes dimensions afin \n",
    "# d'illustrer la différence entre ces deux types. La sortie du Listing 1.6 est probablement\n",
    "# ce à quoi vous vous attendriez et elle est montrée ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf84294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables in TF 2 (Revisited)\n",
    "# Les variables TF 2 peuvent être mises à jour pendant la propagation d'erreur vers\n",
    "# l'arrière (également appelée « backprop », qui est abordée plus loin dans ce livre).\n",
    "# Les variables TF 2 peuvent également être enregistrées puis restaurées ultérieurement. \n",
    "# La liste suivante contient certaines propriétés des variables TF 2 :\n",
    "\n",
    "# • la valeur initiale est facultative\n",
    "# • doit être initialisé avant l'exécution du graphe\n",
    "# • mis à jour pendant l'entrainement\n",
    "# • constamment recalculé\n",
    "# • ils contiennent des valeurs pour les poids et les biais\n",
    "# • tampon en mémoire (sauvegardé/restauré à partir du disque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(3, name='b')\n",
    "x = tf.Variable(2, name='x')\n",
    "z = tf.Variable(5*x, name=\"z\")\n",
    "W = tf.Variable(20)\n",
    "lm = tf.Variable(W*x + b, name=\"lm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other properties of TF 2 variables are listed as follows:\n",
    "\n",
    "# un tenseur qui peut être mis à jour via des opérations\n",
    "# existent en dehors du contexte de session.run\n",
    "# comme une variable \"normale\"\n",
    "# contient les paramètres du modèle appris\n",
    "# les variables peuvent être partagées (ou non formables)\n",
    "# utilisé pour stocker/maintenir l'état\n",
    "# stocke en interne un tenseur persistant\n",
    "# vous pouvez lire/modifier les valeurs du tenseur\n",
    "# plusieurs travailleurs voient les mêmes valeurs pour tf.Variables\n",
    "# la meilleure façon de représenter l'état partagé et persistant manipulé par votre programme\n",
    "\n",
    "# TF 2 propose également la méthode tf.assign() pour modifier les valeurs des variables de TF 2 ; \n",
    "# assurez-vous de lire l'exemple de code correspondant plus loin dans ce chapitre afin d'apprendre \n",
    "# à utiliser correctement cette API.\n",
    "\n",
    "# Gardez à l'esprit la distinction suivante entre les variables TF et les tenseurs TF :\n",
    "# les variables TF représentent les paramètres entrainables de votre modèle (par exemple, \n",
    "# les poids et les biais d'un réseau de neurones), tandis que les tenseurs TF représentent\n",
    "# les données introduites dans votre modèle et les représentations intermédiaires de ces \n",
    "# données comme il passe par votre modèle.\n",
    "\n",
    "# Dans la section suivante, vous découvrirez le \"décorateur\" @tf.function pour les fonctions\n",
    "# Python et comment il peut améliorer les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdcc52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Is @tf.function in TF 2?\n",
    "\n",
    "# TF 2 a introduit le \"décorateur\" @tf.function pour les fonctions Python qui définit un graphe et effectue \n",
    "# l'exécution de la session : c'est en quelque sorte un successeur\" de tf.Session() dans TF 1.x.\n",
    "# Étant donné que les graphes peuvent toujours être utiles, @tf.function convertit de manière transparente \n",
    "# les fonctions Python en fonctions \"soutenues\" par des graphes.\n",
    "# Ce décorateur convertit également le flux de contrôle Python dépendant du tenseur en flux de contrôle TF, \n",
    "# et ajoute également des dépendances de contrôle pour ordonner les opérations de lecture et d'écriture à un \n",
    "# état TF 2. Rappelez-vous que @tf.function fonctionne mieux avec les opérations TF 2 au lieu des opérations \n",
    "# NumPy ou des primitives Python\n",
    "\n",
    "# En général, vous n'aurez pas besoin de décorer les fonctions avec @tf.function ; utilisez-le pour décorer \n",
    "# des calculs de haut niveau, comme une étape de formation ou la passe en avant d'un modèle.\n",
    "\n",
    "# Bien que le mode d'exécution rapide de TF 2 facilite une interface utilisateur plus intuitive, cette facilité\n",
    "# d'utilisation peut se faire au détriment des performances. Heureusement, le décorateur @tf.function est une \n",
    "# technique pour générer des graphes dans le code TF 2 qui s'exécutent plus rapidement que le mode d'exécution \n",
    "# impatient.\n",
    "\n",
    "# L'avantage en termes de performances dépend des types d'opérations effectuées : la multiplication matricielle\n",
    "# ne bénéficie pas de l'utilisation de @tf.function, tandis que l'optimisation d'un réseau de neurones profond peut\n",
    "# bénéficier de @tf.function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Does @tf.function Work?\n",
    "\n",
    "# Chaque fois que vous décorez une fonction Python avec @tf.function, TF 2 construit automatiquement la fonction en mode\n",
    "# graphique.Si une fonction Python décorée avec @tf.function invoque d'autres fonctions Python qui ne sont pas décorées \n",
    "# avec @tf.function, alors le code de ces fonctions Python \"non décorées\" sera également inclus dans le graphique généré.\n",
    "\n",
    "# Un autre point à garder à l'esprit est qu'un tf.Variable en mode impatient est en fait un objet Python \"ordinaire\": cet \n",
    "# objet est détruit lorsqu'il est hors de portée. D'autre part, un objet tf.Variable définit un objet persistant si la  \n",
    "# fonction est décorée via @tf.function. Dans ce scénario, le mode impatient est désactivé et l'objet tf.Variable définit \n",
    "# un nœud dans un graphe TF 2 persistant.Par conséquent, une fonction qui fonctionne en mode impatient sans annotation peut\n",
    "# échouer lorsqu'elle est décorée avec @tf.function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe56b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# A Caveat about @tf.function in TF \n",
    "\n",
    "# Si des constantes sont définies avant la définition d'une fonction Python décorée, vous pouvez imprimer leurs valeurs dans la\n",
    "# fonction en utilisant la fonction Python print(). D'un autre côté, si des constantes sont définies dans la définition d'une \n",
    "# fonction Python décorée, vous pouvez imprimer leurs valeurs dans la fonction en utilisant la fonction TF 2 tf.print(). \n",
    "# Considérez ce bloc de code :\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.add(4, 2)\n",
    "\n",
    "@tf.function\n",
    "def compute_values():\n",
    "    print(a) # 6\n",
    "    \n",
    "compute_values()\n",
    "# output:\n",
    "# tf.Tensor(6, shape=(), dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57a41c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Comme vous pouvez le voir, le résultat correct est affiché (en gras). Cependant, si vous définissez des constantes dans \n",
    "# une fonction Python décorée, la sortie contient des types et des attributs mais pas l'exécution de l'opération d'addition.\n",
    "# Considérez le bloc de code suivant :\n",
    "\n",
    "@tf.function\n",
    "def compute_values():\n",
    "    a = tf.add(4, 2)\n",
    "    print(a)\n",
    "    \n",
    "compute_values()\n",
    "# output:\n",
    "# Tensor(\"Add:0\", shape=(), dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd24f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Le zéro dans la sortie précédente fait partie du nom du tenseur et non d'une valeur sortie. Plus précisément, Add:0 est la \n",
    "# sortie zéro de l'opération tf.add(). Toute invocation supplémentaire de compute_values() n'imprime rien. Si vous voulez des\n",
    "# résultats réels, une solution consiste à renvoyer une valeur de la fonction, comme indiqué ici :\n",
    "\n",
    "@tf.function\n",
    "def compute_values():\n",
    "    a=tf.add(2,4)\n",
    "    return a\n",
    "\n",
    "result = compute_values()\n",
    "print(\"result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acbdb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "result: tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# A second solution involves the TF tf.print() function instead of the Python print() function, as shown in bold in this code\n",
    "# block:\n",
    "\n",
    "@tf.function\n",
    "def compute_values_1():\n",
    "    a=tf.add(2,4)\n",
    "    tf.print(a)\n",
    "    \n",
    "# Une troisième solution consiste à convertir les valeurs numériques en Tensors si elles n'affectent pas la forme du graphique\n",
    "# généré, comme illustré ici :\n",
    "\n",
    "@tf.function\n",
    "def compute_values():\n",
    "    a = tf.add(tf.constant(4), tf.constant(2))\n",
    "    return a\n",
    "\n",
    "\n",
    "compute_values_1()\n",
    "\n",
    "result = compute_values()\n",
    "print(\"result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ee6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tf.print() Function and Standard Error\n",
    "\n",
    "# Il y a encore un détail à retenir : la fonction Python print() « envoie » la sortie à quelque chose appelé « sortie standard »\n",
    "# qui est associée à un descripteur de fichier dont la valeur est 1 ; d'autre part, tf.print() envoie la sortie à \"l'erreur \n",
    "# standard\" qui est associée à un descripteur de fichier dont la valeur est 2. Dans les langages de programmation tels que C, \n",
    "# seules les erreurs sont envoyées à l'erreur standard, alors gardez à l'esprit que le comportement de tf.print() diffère de la \n",
    "# convention concernant la sortie standard et l'erreur standard. Les extraits de code suivants illustrent cette différence :\n",
    "\n",
    " # python3 file_with_print.py 1>print_output\n",
    " # python3 file_with_tf.print.py 2>tf.print_output\n",
    "\n",
    "# Si votre fichier Python contient à la fois print() et tf.print(), vous pouvez capturer la sortie comme suit :\n",
    "\n",
    "# python3 both_prints.py 1>print_output 2>tf.print_output\n",
    "\n",
    "# Cependant, gardez à l'esprit que l'extrait de code précédent peut également rediriger les messages d'erreur réels vers le \n",
    "# fichier tf.print_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with @tf.function in TF 2\n",
    "\n",
    "# La section précédente a expliqué comment la sortie différera selon que vous utilisez la fonction Python print() par rapport à \n",
    "# la fonction tf.print() dans le code TF 2, où cette dernière fonction envoie également la sortie à l'erreur standard au lieu de\n",
    "# la sortie standard.\n",
    "\n",
    "# Cette section contient plusieurs exemples du décorateur @tf.function dans TF 2 pour vous montrer quelques nuances de comportement\n",
    "# qui dépendent de l'endroit où vous définissez les constantes et si vous utilisez la fonction tf.print() ou la fonction Python\n",
    "# print(). Gardez également à l'esprit les commentaires de la section précédente concernant @tf.function, ainsi que le fait que \n",
    "# vous n'avez pas besoin d'utiliser @tf.function dans toutes vos fonctions Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22edf0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10. 10.]\n",
      " [11.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# An Example without @tf.function\n",
    "\n",
    "# Le Listing 1.7 affiche le contenu de tf2_simple_function.py, qui illustre comment définir une fonction Python avec du \n",
    "# code TF 2.\n",
    "\n",
    "# Listing 1.7: tf2_simple_function.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def func():\n",
    "    a = tf.constant([[10,10],[11.,1.]])\n",
    "    b = tf.constant([[1.,0.],[0.,1.]])\n",
    "    c = tf.matmul(a, b)\n",
    "    return c\n",
    "\n",
    "print(func().numpy())\n",
    "\n",
    "# Le code du Listing 1.7 est simple : une fonction Python func() définit deux constantes TF 2 , calcule leur produit et renvoie \n",
    "# cette valeur. Étant donné que TF 2 fonctionne en mode impatient par défaut, la fonction Python func() est traitée comme une \n",
    "# fonction \"normale\". Lancez le code et vous verrez la sortie suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90360df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10. 10.]\n",
      " [11.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# An Example with @tf.function\n",
    "\n",
    "# Le Listing 1.8 affiche le contenu de tf2_at_function.py, qui illustre comment définir une fonction Python décorée avec du\n",
    "# code TF\n",
    "\n",
    "# Listing 1.8: tf2_at_function.py\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def func():\n",
    "    a = tf.constant([[10,10],[11.,1.]])\n",
    "    b = tf.constant([[1.,0.],[0.,1.]])\n",
    "    c = tf.matmul(a, b)\n",
    "    return c\n",
    "\n",
    "print(func().numpy())\n",
    "\n",
    "# Le Listing 1.8 définit une fonction Python décorée : le reste du code est identique au Listing 1.7. Cependant, à cause de\n",
    "# l'annotation @tf.function, la fonction Python func() est \"encapsulée\" dans un tensorflow.python. objet impatient.def_function.\n",
    "# Function. La fonction Python est affectée à la propriété python_function de l'objet.\n",
    "\n",
    "# Lorsque func() est appelée, la construction du graphe commence. Seul le code Python est exécuté, et le comportement de la \n",
    "# fonction est tracé afin que TF 2 puisse collecter les données requises pour construire le graphe. La sortie est affichée \n",
    "# ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fb9601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add 1:  tf.Tensor(2, shape=(), dtype=int32)\n",
      "Add 2.3:  tf.Tensor(4.6, shape=(), dtype=float32)\n",
      "Add string tensor: tf.Tensor(b'abcabc', shape=(), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'aa'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overloading Functions with @tf.function\n",
    "\n",
    "# Si vous avez travaillé avec des langages de programmation tels que Java et C++, vous connaissez déjà le concept de \"surcharger\"\n",
    "# une fonction. Si ce terme est nouveau pour vous, l'idée est simple : une fonction surchargée est une fonction qui peut être\n",
    "# invoquée avec différents types de données. Par exemple, vous pouvez définir une fonction \"add\" surchargée qui peut ajouter \n",
    "# deux nombres ainsi que \"additionner\" (c'est-à-dire concaténer) deux chaînes.\n",
    "\n",
    "# Si vous êtes curieux, les fonctions surchargées dans divers langages de programmation sont implémentées via la \"mangling de \n",
    "# noms\", ce qui signifie que la signature (les paramètres et leurs types de données pour la fonction) est ajoutée au nom de la\n",
    "# fonction afin de générer un nom de fonction unique . Cela se passe « sous le capot », ce qui signifie que vous n'avez pas à\n",
    "# vous soucier des détails de mise en œuvre.\n",
    "\n",
    "# Le Listing 1.9 affiche le contenu de tf2_overload.py, qui illustre comment définir une fonction Python décorée pouvant être \n",
    "# invoquée avec différents types de données.\n",
    "\n",
    "# Listing 1.9: tf2_overload.py\n",
    "\n",
    "@tf.function\n",
    "def add(a):\n",
    "    return a + a \n",
    "\n",
    "print(\"Add 1: \", add(1))\n",
    "print(\"Add 2.3: \", add(2.3))\n",
    "print(\"Add string tensor:\", add(tf.constant(\"abc\")))\n",
    "\n",
    "c = add.get_concrete_function(tf.TensorSpec(shape=None,dtype=tf.string))\n",
    "c(a=tf.constant(\"a\")) \n",
    "\n",
    "# Le Listing 1.9 définit une fonction Python décorée add(), qui est précédée d'un décorateur @tf.function. Cette fonction peut\n",
    "# être invoquée en passant un nombre entier, une valeur décimale ou un tenseur TF 2, et le résultat correct est calculé. Lancez\n",
    "# le code et vous verrez la sortie suivante :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Is AutoGraph in TF 2?\n",
    "\n",
    "# AutoGraph fait référence à la conversion du code Python en sa représentation graphique, qui est une nouvelle fonctionnalité \n",
    "# importante dans TF 2. En fait, AutoGraph est automatiquement appliqué aux fonctions décorées avec @tf.function ; ce décorateur \n",
    "# crée des graphes appelables à partir de fonctions Python.\n",
    "\n",
    "# AutoGraph transforme un sous-ensemble de la syntaxe Python en sa représentation graphique portable, hautes performances et \n",
    "# indépendante du langage, comblant ainsi l'écart entre TF 1.x et TF 2.0. En fait, AutoGraph vous permet d'inspecter son code \n",
    "# généré automatiquement avec cet extrait de code. Par exemple, si vous définissez une fonction Python appelée my_product(), \n",
    "# vous pouvez inspecter son code généré automatiquement avec cet extrait :\n",
    "\n",
    " print(tf.autograph.to_code(my_product))\n",
    "    \n",
    "# En particulier, la construction Python for/while est implémentée dans TF 2 via tf.while_loop (break et continue sont également\n",
    "# pris en charge). La construction Python if est implémentée dans TF 2 via tf.cond. Le \"for _ in dataset\" est implémenté dans \n",
    "# TF 2 via dataset.reduce.   \n",
    "\n",
    "# AutoGraph a également quelques règles pour convertir les boucles. Une boucle for est convertie si l'itérable dans la boucle \n",
    "# est un tenseur, et une boucle while est convertie si la condition while dépend d'un tenseur. Si une boucle est convertie, \n",
    "# elle sera « déroulée » dynamiquement avec tf.while_loop, ainsi que le cas particulier d'un for x dans tf.data.Dataset \n",
    "# (ce dernier est transformé en tf.data.Dataset.reduce). Si une boucle n'est pas convertie, elle sera déroulée statiquement.\n",
    "\n",
    "# AutoGraph prend en charge le flux de contrôle qui est imbriqué de manière arbitraire, vous pouvez donc implémenter de nombreux\n",
    "# types de programmes ML. Consultez la documentation en ligne pour plus d'informations sur AutoGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73a823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "-2\n",
      "18\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic Operations in TF 2\n",
    "\n",
    "# Le Listing 1.10 affiche le contenu de tf2_arithmetic.py, qui illustre comment effectuer des opérations arithmétiques dans \n",
    "# TF 2.\n",
    "\n",
    "# Listing 1.10: tf2_arithmetic.py\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "@tf.function # repłace print() with tf.print()\n",
    "def compute_values():\n",
    "    a = tf.add(4, 2)\n",
    "    b = tf.subtract(8,6)\n",
    "    c = tf.multiply(a, 3)\n",
    "    d = tf.math.divide(a, 6)\n",
    "    tf.print(a) # 6\n",
    "    tf.print(b) # 2\n",
    "    tf.print(c) # 18\n",
    "    tf.print(d) # 1\n",
    "    \n",
    "compute_values()\n",
    "\n",
    "# Le Listing 1.10 définit la fonction Python décorée compute_values() avec un code simple pour calculer la somme, la différence,\n",
    "# le produit et le quotient de deux nombres via tf.add(), tf.subtract(), tf.multiply() et tf API .math.divide(), respectivement.\n",
    "# Les quatre instructions print() affichent les valeurs de a, b, c et d. La sortie du Listing 1.10 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd294fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: tf.Tensor([3. 2.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Caveats for Arithmetic Operations in TF 2\n",
    "\n",
    "# Comme vous pouvez probablement le supposer, vous pouvez également effectuer des opérations arithmétiques impliquant des \n",
    "# constantes et des variables TF 2 . Le Listing 1.11 affiche le contenu de tf2_const_var.py, qui illustre comment effectuer\n",
    "# des opérations arithmétiques impliquant une constante TF 2 et une variable.\n",
    "\n",
    "# Listing 1.11: tf2_const_var.py\n",
    "\n",
    "v1 = tf.Variable([4.0, 4.0])\n",
    "c1 = tf.constant([1.0, 2.0])\n",
    "\n",
    "diff = tf.subtract(v1,c1)\n",
    "print(\"diff:\",diff)\n",
    "\n",
    "# Le Listing 1.11 calcule la différence entre la variable TF v1 et la constante TF c1, et la sortie est affichée ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c07c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff1: [3. 2.]\n",
      "diff2: [3. 2.]\n",
      "diff3: [ 9. 18.]\n"
     ]
    }
   ],
   "source": [
    "# Cependant, si vous mettez à jour la valeur de v1 puis imprimez la valeur de diff, cela ne changera pas. Vous devez \n",
    "# réinitialiser la valeur de diff, comme vous le feriez dans d'autres langages de programmation impératifs.\n",
    "\n",
    "# Le Listing 1.12 affiche le contenu de tf2_const_var2.py, qui illustre comment effectuer des opérations arithmétiques\n",
    "# impliquant une constante TF 2 et une variable.\n",
    "\n",
    "# Listing 1.12: tf2_const_var2.py\n",
    "\n",
    "v1 = tf.Variable([4.0, 4.0])\n",
    "c1 = tf.constant([1.0, 2.0])\n",
    "diff = tf.subtract(v1,c1)\n",
    "print(\"diff1:\",diff.numpy())\n",
    "\n",
    "# diff is NOT updated:\n",
    "v1.assign([10.0, 20.0])\n",
    "print(\"diff2:\",diff.numpy())\n",
    "\n",
    "# diff is updated correctly:\n",
    "diff = tf.subtract(v1,c1)\n",
    "print(\"diff3:\",diff.numpy())\n",
    "\n",
    "# Le Listing 1.12 recalcule la valeur de diff dans la partie finale du Listing 1.11, après quoi il a la valeur correcte. \n",
    "# La sortie est affichée ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d3902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "2\n",
      "6.27832947e-07\n",
      "-1\n",
      "0.999999642\n"
     ]
    }
   ],
   "source": [
    "# TF 2 and Built-In Functions\n",
    "\n",
    "# Le Listing 1.13 affiche le contenu de tf2_math_ops.py, qui illustre comment effectuer des opérations arithmétiques \n",
    "# supplémentaires dans un graphe TF.\n",
    "\n",
    "# Listing 1.13: tf2_math_ops.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "PI = 3.141592\n",
    "@tf.function # repłace print() with tf.print()\n",
    "def math_values():\n",
    "    tf.print(tf.math.divide(12,8))\n",
    "    tf.print(tf.math.floordiv(20.0,8.0))\n",
    "    tf.print(tf.sin(PI))\n",
    "    tf.print(tf.cos(PI))\n",
    "    tf.print(tf.math.divide(tf.sin(PI/4.), tf.cos(PI/4.)))\n",
    "math_values()\n",
    "\n",
    "# Le Listing 1.13 contient une approximation codée en dur pour PI, suivie de la fonction Python décorée math_values() avec \n",
    "# cinq instructions print() qui affichent divers résultats arithmétiques. Notez en particulier que la troisième valeur de\n",
    "# sortie est un très petit nombre (la valeur correcte est zéro). La sortie de l'extrait 1.13 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac378539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "2\n",
      "-8.74227766e-08\n",
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Le Listing 1.14 affiche le contenu de tf2_math-ops_pi.py, qui illustre comment effectuer des opérations arithmétiques \n",
    "# dans TF 2\n",
    "\n",
    "# Listing 1.14: tf2_math_ops_pi.py\n",
    " \n",
    "import math as m\n",
    "PI = tf.constant(m.pi)\n",
    "\n",
    "@tf.function # repłace print() with tf.print()\n",
    "def math_values():\n",
    "    tf.print(tf.math.divide(12,8))\n",
    "    tf.print(tf.math.floordiv(20.0,8.0))\n",
    "    tf.print(tf.sin(PI))\n",
    "    tf.print(tf.cos(PI))\n",
    "    tf.print(tf.math.divide(tf.sin(PI/4.), tf.cos(PI/4.)))\n",
    "    \n",
    "math_values()\n",
    "\n",
    "# Le Listing 1.14 est presque identique au code du Listing 1.13 : la seule différence est que le Listing 1.13 spécifie une\n",
    "# valeur codée en dur pour PI, tandis que le Listing 1.14 attribue m.pi à la valeur de PI. Par conséquent, la valeur approchée\n",
    "# est une décimale plus proche de la valeur correcte de zéro. La sortie de l'extrait 1.14 est ici ; remarquez comment le format\n",
    "# de sortie diffère du Listing 1.13 en raison de la fonction Python print() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d396f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(0.49999997, shape=(), dtype=float32)\n",
      "b: tf.Tensor(0.86602545, shape=(), dtype=float32)\n",
      "c: tf.Tensor(2.0000002, shape=(), dtype=float32)\n",
      "d: tf.Tensor(0.57735026, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Trigonometric Values in TF\n",
    "\n",
    "# Le Listing 1.15 affiche le contenu de tf2_trig_values.py, qui illustre comment calculer des valeurs impliquant des fonctions\n",
    "# trigonométriques dans TF 2.\n",
    "\n",
    "# Listing 1.15: tf2_trig_values.py\n",
    "\n",
    "import math as m\n",
    "\n",
    "PI = tf.constant(m.pi)\n",
    "a = tf.cos(PI/3.)\n",
    "b = tf.sin(PI/3.)\n",
    "c = 1.0/a # sec(60)\n",
    "d = 1.0/tf.tan(PI/3.) # cot(60)\n",
    "\n",
    "@tf.function # this decorator is okay\n",
    "def math_values():\n",
    "    print(\"a:\",a)\n",
    "    print(\"b:\",b)\n",
    "    print(\"c:\",c)\n",
    "    print(\"d:\",d)\n",
    "\n",
    "math_values()\n",
    "\n",
    "# Le Listing 1.14 est simple : il existe plusieurs des mêmes API TF 2 que celles que vous avez vues dans le Listing 1.13.\n",
    "# De plus, le Listing 1.14 contient l'API tf.tan(), qui calcule la tangente d'un nombre (en radians). La sortie de l'extrait \n",
    "# 1.14 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e500ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tf.Tensor(2.7182817, shape=(), dtype=float32)\n",
      "b:  tf.Tensor(0.13533528, shape=(), dtype=float32)\n",
      "s1: tf.Tensor(0.8807971, shape=(), dtype=float32)\n",
      "s2: tf.Tensor(0.880797, shape=(), dtype=float32)\n",
      "t2: tf.Tensor(0.9640276, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Exponential Values in TF 2\n",
    "\n",
    "# Le Listing 1.15 affiche le contenu de tf2_exp_values.py, qui illustre comment calculer des valeurs impliquant des fonctions \n",
    "# trigonométriques supplémentaires dans TF 2.\n",
    "\n",
    "# Listing 1.15: tf2_exp_values.py\n",
    "\n",
    "a = tf.exp(1.0)\n",
    "b = tf.exp(-2.0)\n",
    "s1 = tf.sigmoid(2.0)\n",
    "s2 = 1.0/(1.0 + b)\n",
    "t2 = tf.tanh(2.0)\n",
    "\n",
    "@tf.function # this decorator is okay\n",
    "def math_values():\n",
    "    print('a: ', a)\n",
    "    print('b: ', b)\n",
    "    print('s1:', s1)\n",
    "    print('s2:', s2)\n",
    "    print('t2:', t2)\n",
    "    \n",
    "math_values()\n",
    "\n",
    "# Le listing 1.15 commence par les API TF 2 tf.exp(), tf.sigmoid() et tf.tanh() qui calculent la valeur exponentielle d'un \n",
    "# nombre, la valeur sigmoïde d'un nombre et la tangente hyperbolique d'un nombre, respectivement. La sortie de l'extrait \n",
    "# 1.15 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ab74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tf.Tensor(b'caf\\xc3\\xa9', shape=(), dtype=string)\n",
      "\n",
      "len1: 4\n",
      "len2: [ 99  97 102 233]\n",
      "\n",
      "x2: tf.Tensor([b'Caf\\xc3\\xa9' b'Coffee' b'caff\\xc3\\xa8' b'\\xe5\\x92\\x96\\xe5\\x95\\xa1'], shape=(4,), dtype=string)\n",
      "\n",
      "len2: [4 6 5 2]\n",
      "\n",
      "r: <tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>\n"
     ]
    }
   ],
   "source": [
    "# Working with Strings in TF 2\n",
    "\n",
    "# Le Listing 1.16 affiche le contenu de tf2_strings.py, qui illustre comment travailler avec des chaînes dans TF 2.\n",
    "\n",
    "# Listing 1.16: tf2_strings.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.constant(\"café\")\n",
    "\n",
    "print(\"x1:\",x1)\n",
    "\n",
    "tf.strings.length(x1)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "len1 = tf.strings.length(x1,unit=\"UTF8_CHAR\")\n",
    "\n",
    "len2 = tf.strings.unicode_decode(x1,\"UTF8\")\n",
    "\n",
    "print(\"len1:\",len1.numpy())\n",
    "print(\"len2:\",len2.numpy())\n",
    "print(\"\")\n",
    "\n",
    "# String arrays\n",
    "x2 = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])\n",
    "\n",
    "print(\"x2:\",x2)\n",
    "print(\"\")\n",
    "\n",
    "len3 = tf.strings.length(x2, unit=\"UTF8_CHAR\")\n",
    "print(\"len2:\",len3.numpy())\n",
    "print(\"\")\n",
    "r = tf.strings.unicode_decode(x2, \"UTF8\")\n",
    "print(\"r:\",r)\n",
    "\n",
    "# Le Listing 1.16 définit la constante TF 2 x1 comme une chaîne contenant un accent. La première instruction print() affiche les\n",
    "# trois premiers caractères de x1, suivis d'une paire de valeurs hexadécimales qui représentent le caractère « e » accentué. Les\n",
    "# deuxième et troisième instructions print() affichent le nombre de caractères dans x1, suivi de la séquence UTF8 pour la chaîne\n",
    "# x1.\n",
    "\n",
    "# La partie suivante du Listing 1.16 définit la constante TF 2 x2 comme un tenseur TF 2 de premier ordre qui contient quatre \n",
    "# chaînes. L'instruction print() suivante affiche le contenu de x2, en utilisant les valeurs UTF8 pour les caractères contenant\n",
    "# des accents.\n",
    "\n",
    "# La dernière partie du Listing 1.16 définit r comme les valeurs Unicode pour les caractères de la chaîne x2. La sortie de l'extrait\n",
    "# 1.14 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le chapitre 2 contient un exemple de code complet avec plus d'exemples d'un RaggedTensor dans TF 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f25202e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "x.shape: (2, 3)\n",
      "\n",
      "x.dtype: <dtype: 'float32'>\n",
      "\n",
      "x[:,1:] : tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "x[..., 1, tf.newaxis]: tf.Tensor(\n",
      "[[2.]\n",
      " [5.]], shape=(2, 1), dtype=float32)\n",
      "\n",
      "x + 10 : tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "tf.square(x) tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "x @ tf.transpose(x) : tf.Tensor(\n",
      "[[14. 32.]\n",
      " [32. 77.]], shape=(2, 2), dtype=float32)\n",
      "m1         : tf.Tensor(\n",
      "[[ 1.  2.  4.]\n",
      " [ 3.  6. 12.]], shape=(2, 3), dtype=float32)\n",
      "m1 + 50         : tf.Tensor(\n",
      "[[51. 52. 54.]\n",
      " [53. 56. 62.]], shape=(2, 3), dtype=float32)\n",
      "m1 * 2        : tf.Tensor(\n",
      "[[ 2.  4.  8.]\n",
      " [ 6. 12. 24.]], shape=(2, 3), dtype=float32)\n",
      "tf.square(m1):  tf.Tensor(\n",
      "[[  1.   4.  16.]\n",
      " [  9.  36. 144.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Working with Tensors and Operations in TF 2\n",
    "\n",
    "# Le Listing 1.17 affiche le contenu de tf2_tensors_operations.py, qui illustre comment utiliser divers opérateurs avec des \n",
    "# tenseurs dans TF 2.\n",
    "\n",
    "# Listing 1.17: tf2_tensors_operations.py\n",
    "\n",
    "x = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"\")\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"\")\n",
    "\n",
    "print(\"x.dtype:\", x.dtype)\n",
    "print(\"\")\n",
    "print(\"x[:,1:] :\",x[:,1:])\n",
    "print(\"\")\n",
    "print(\"x[..., 1, tf.newaxis]:\", x[..., 1, tf.newaxis])\n",
    "print(\"\")\n",
    "print(\"x + 10 :\", x + 10)\n",
    "print(\"\")\n",
    "print(\"tf.square(x)\",tf.square(x))\n",
    "print(\"\")\n",
    "print(\"x @ tf.transpose(x) :\",x @ tf.transpose(x))\n",
    "\n",
    "\n",
    "m1 = tf.constant([[1., 2., 4.], [3., 6., 12.]])\n",
    "print(\"m1         :\", m1)\n",
    "print(\"m1 + 50         :\", m1 + 50)\n",
    "print(\"m1 * 2        :\", m1 * 2)\n",
    "print(\"tf.square(m1): \", tf.square(m1))\n",
    "\n",
    "# Le Listing 1.17 définit le tenseur TF x qui contient un tableau 2x3 de nombres réels. La majeure partie du code du Listing \n",
    "# 1.17 illustre comment afficher les propriétés de x en appelant x.shape et x.dtype, ainsi que la fonction TF tf.square(x).\n",
    "# La sortie de l'extrait 1.17 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d104f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr2:  [[1 2]\n",
      " [2 3]]\n",
      "[0]:  [1 2]\n",
      "[1]:  [2 3]\n"
     ]
    }
   ],
   "source": [
    "# Second-Order Tensors in TF 2 (1)\n",
    "\n",
    "# Le Listing 1.18 affiche le contenu de tf2_elem2.py, qui illustre comment définir un tenseur TF de second ordre et accéder aux\n",
    "# éléments de ce tenseur.\n",
    "\n",
    "# Listing 1.18: tf2_elem2.py\n",
    "\n",
    "arr2 = tf.constant([[1,2],[2,3]])\n",
    "\n",
    "@tf.function\n",
    "def compute_values():\n",
    "    tf.print('arr2: ',arr2)\n",
    "    tf.print('[0]: ',arr2[0])\n",
    "    tf.print('[1]: ',arr2[1])\n",
    "    \n",
    "compute_values() \n",
    "\n",
    "# Le Listing 1.18 contient la constante TF arr1 qui est initialisée avec la valeur [[1,2],[2,3]]. Les trois instructions print()\n",
    "# affichent la valeur de arr1, la valeur de l'élément dont l'index est 1 et la valeur de l'élément dont l'index est [1,1]. La \n",
    "# sortie de l'extrait 1.18 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55760593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr3:  [[[1 2]\n",
      "  [2 3]]\n",
      "\n",
      " [[3 4]\n",
      "  [5 6]]]\n",
      "\n",
      "[1]:  [[3 4]\n",
      " [5 6]]\n",
      "\n",
      "[1,1]:  [5 6]\n",
      "\n",
      "[1,1,0]: 5\n"
     ]
    }
   ],
   "source": [
    "# Second-Order Tensors in TF 2 (2)\n",
    "\n",
    "# Le Listing 1.19 affiche le contenu de tf2_elem3.py, qui illustre comment définir un tenseur TF 2 de second ordre et accéder\n",
    "# aux éléments de ce tenseur.\n",
    "\n",
    "# Listing 1.19: tf2_elem3.py\n",
    "\n",
    "arr3 = tf.constant([[[1,2],[2,3]],[[3,4],[5,6]]])\n",
    "\n",
    "@tf.function # replace print() with tf.print()\n",
    "def compute_values():\n",
    "    tf.print('arr3: ',arr3)\n",
    "    tf.print(\"\")\n",
    "    tf.print('[1]: ',arr3[1])\n",
    "    tf.print(\"\")\n",
    "    tf.print('[1,1]: ',arr3[1,1])\n",
    "    tf.print(\"\")\n",
    "    tf.print('[1,1,0]:',arr3[1,1,0])\n",
    "\n",
    "compute_values()\n",
    "\n",
    "# Le Listing 1.19 contient la constante TF arr3 qui est initialisée avec la valeur [[[1,2],[2,3]],[[3,4],[5,6]]]. Les quatre\n",
    "# instructions print() affichent la valeur de arr3, la valeur de l'élément dont l'index est 1, la valeur de l'élément dont \n",
    "# l'index est [1,1] et la valeur de l'élément dont l'index est [1,1, 0]. La sortie de l'extrait 1.19 (légèrement ajustée à des \n",
    "# fins d'affichage) est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e9f460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1: tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)\n",
      "m2: tf.Tensor(\n",
      "[[2.]\n",
      " [2.]], shape=(2, 1), dtype=float32)\n",
      "p1: tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Multiplying Two Second-Order Tensors in TF\n",
    "\n",
    "# Le Listing 1.20 affiche le contenu de tf2_mult.py, qui illustre comment multiplier les tenseurs du second ordre dans TF 2.\n",
    "\n",
    "# Listing 1.20: tf2_mult.py\n",
    "\n",
    "m1 = tf.constant([[3., 3.]]) # 1x2\n",
    "m2 = tf.constant([[2.],[2.]]) # 2x1\n",
    "p1 = tf.matmul(m1, m2) # 1x1\n",
    "\n",
    "@tf.function\n",
    "def compute_values():\n",
    "    print('m1:',m1) #tf.print('m1:',m1[0])\n",
    "    print('m2:',m2) #tf.print('m2:',m2[0])\n",
    "    print('p1:',p1) #tf.print('p1:',p1[0])\n",
    "\n",
    "compute_values()   \n",
    "\n",
    "# Le Listing 1.20 contient deux constantes TF m1 et m2 qui sont initialisées avec les valeurs [[3., 3.]] et [[2.],[2.]]. En \n",
    "# raison des crochets imbriqués, m1 a la forme 1x2, tandis que m2 a la forme 2x1. Par conséquent, le produit de m1 et m2 a la\n",
    "# forme (1,1).\n",
    "\n",
    "# Les trois instructions print() affichent les valeurs de m1, m2 et p1. La sortie de l'extrait 1.20 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "447feb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "x2: tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert Python Arrays to TF Tensors\n",
    "\n",
    "# Le Listing 1.21 affiche le contenu de tf2_convert_tensors.py,qui illustre comment convertir un tableau Python\n",
    "# en un tenseur TF 2.\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "x1 = np.array([[1.,2.],[3.,4.]])\n",
    "x2=tf.convert_to_tensor(value=x1, dtype=tf.float32)\n",
    "\n",
    "print ('x1:',x1)\n",
    "print(\"\")\n",
    "print ('x2:',x2)\n",
    "\n",
    "# Le listing 1.21 est simple, en commençant par une déclaration d'importation pour TensorFlow et une pour NumPy. Ensuite,\n",
    "# la variable x_data est un tableau NumPy et x est un tenseur TF résultant de la conversion de x_data en un tenseur TF.  \n",
    "# La sortie de l'extrait 1.21 est ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "027c62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]\n",
      "tf.Tensor(2.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Conflicting Types in TF 2\n",
    "\n",
    "# Le Listing 1.22 affiche le contenu de tf2_conflict_types.py, qui illustre ce qui se passe lorsque vous essayez de combiner des\n",
    "# tenseurs incompatibles dans TF 2.\n",
    "\n",
    "# Listing 1.22: tf2_conflict_types.py\n",
    "try:\n",
    "    print(tf.constant(1) + tf.constant(1.0))\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "\n",
    "try:\n",
    "    print(tf.constant(1) + tf.constant(1))\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "    \n",
    "    \n",
    "\n",
    "try:\n",
    "    print(tf.constant(1.0, dtype=tf.float64) + tf.constant(1.0))\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "    \n",
    "try:\n",
    "    print(tf.constant(1.0, dtype=tf.float64) + tf.constant(1.0 , dtype=tf.float64))\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "    \n",
    "    \n",
    "# Le Listing 1.22 contient deux blocs try/except. Le premier bloc ajoute deux constantes 1 et 1.0, qui sont compatibles.\n",
    "# Le deuxième bloc tente d'ajouter la valeur 1.0 qui est déclarée comme tf.float64 avec 1.0, qui ne sont pas des tenseurs\n",
    "# compatibles. La sortie de l'extrait 1.22 est ici :   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379671c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
